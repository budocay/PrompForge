"""
Prompt quality analysis for PromptForge web interface.
Evaluates prompts and compares before/after reformatting.

IMPORTANT: Les m√©triques sont bas√©es sur:
1. Des mesures directes (tokens, caract√®res, structure)
2. Des pr√©dictions sourc√©es (√©tudes publi√©es)
3. Des preuves officielles (documentation Anthropic, OpenAI, Google)

Aucun chiffre n'est invent√©. Voir RESEARCH_SOURCES.md pour les sources.
"""

import re
from ..tokens import estimate_tokens
from .template_helpers import TEMPLATE_INFO


def analyze_prompt_quality(prompt: str) -> dict:
    """
    Analyse la qualit√© d'un prompt selon plusieurs crit√®res.
    Retourne un dict avec scores (0-100) et d√©tails.

    Crit√®res √©valu√©s:
    - clarity: Instructions claires et directes
    - context: Pr√©sence de contexte suffisant
    - specificity: Niveau de d√©tail et pr√©cision
    - structure: Organisation et formatage
    - output_format: Format de sortie sp√©cifi√©
    - constraints: Contraintes/limites d√©finies
    - examples: Pr√©sence d'exemples
    """
    prompt_lower = prompt.lower()
    prompt_len = len(prompt)

    scores = {}
    details = {}

    # ==========================================================================
    # 1. CLART√â (0-100) - Instructions directes, pas de vague
    # ==========================================================================
    clarity_score = 50  # Base
    clarity_notes = []

    # Verbes d'action directs
    action_verbs = ['√©cris', 'cr√©e', 'g√©n√®re', 'analyse', 'r√©sume', 'explique',
                   'liste', 'compare', 'traduis', 'corrige', 'optimise', 'refactor',
                   'write', 'create', 'generate', 'analyze', 'summarize', 'explain',
                   'list', 'compare', 'translate', 'fix', 'optimize', 'implement']
    if any(v in prompt_lower for v in action_verbs):
        clarity_score += 20
        clarity_notes.append("‚úÖ Verbes d'action clairs")
    else:
        clarity_notes.append("‚ùå Manque de verbes d'action directs")

    # Mots vagues
    vague_words = ['peut-√™tre', 'possiblement', '√©ventuellement', 'quelque chose',
                   'maybe', 'possibly', 'something', 'stuff', 'things', 'etc']
    vague_count = sum(1 for w in vague_words if w in prompt_lower)
    if vague_count == 0:
        clarity_score += 15
        clarity_notes.append("‚úÖ Pas de termes vagues")
    else:
        clarity_score -= vague_count * 5
        clarity_notes.append(f"‚ö†Ô∏è {vague_count} terme(s) vague(s)")

    # Longueur minimale
    if prompt_len > 100:
        clarity_score += 15
        clarity_notes.append("‚úÖ Longueur suffisante")
    elif prompt_len < 30:
        clarity_score -= 10
        clarity_notes.append("‚ùå Prompt trop court")

    scores['clarity'] = min(100, max(0, clarity_score))
    details['clarity'] = clarity_notes

    # ==========================================================================
    # 2. CONTEXTE (0-100) - Informations de fond fournies
    # ==========================================================================
    context_score = 30  # Base faible
    context_notes = []

    # Balises de contexte explicites
    context_markers = ['<context>', 'contexte:', 'context:', 'background:',
                       'situation:', '## contexte', '## context', 'given:']
    if any(m in prompt_lower for m in context_markers):
        context_score += 40
        context_notes.append("‚úÖ Section contexte explicite")

    # Mentions de projet/stack technique
    tech_context = ['projet', 'project', 'application', 'syst√®me', 'system',
                   'stack', 'architecture', 'database', 'api', 'frontend', 'backend']
    if any(t in prompt_lower for t in tech_context):
        context_score += 20
        context_notes.append("‚úÖ Contexte technique mentionn√©")

    # Longueur comme proxy de contexte
    if prompt_len > 500:
        context_score += 15
        context_notes.append("‚úÖ Contexte d√©taill√© (longueur)")
    elif prompt_len < 100:
        context_notes.append("‚ö†Ô∏è Peu de contexte fourni")

    if context_score <= 30:
        context_notes.append("‚ùå Contexte insuffisant")

    scores['context'] = min(100, max(0, context_score))
    details['context'] = context_notes

    # ==========================================================================
    # 3. SP√âCIFICIT√â (0-100) - Niveau de d√©tail
    # ==========================================================================
    specificity_score = 40  # Base
    specificity_notes = []

    # Nombres et m√©triques sp√©cifiques
    numbers = re.findall(r'\d+', prompt)
    if len(numbers) >= 2:
        specificity_score += 20
        specificity_notes.append(f"‚úÖ M√©triques sp√©cifiques ({len(numbers)} nombres)")

    # Termes techniques
    tech_terms = ['mongodb', 'postgresql', 'react', 'vue', 'python', 'java',
                 'fastapi', 'django', 'express', 'typescript', 'docker', 'kubernetes',
                 'json', 'xml', 'csv', 'rest', 'graphql', 'oauth', 'jwt']
    tech_count = sum(1 for t in tech_terms if t in prompt_lower)
    if tech_count >= 2:
        specificity_score += 25
        specificity_notes.append(f"‚úÖ Termes techniques pr√©cis ({tech_count})")
    elif tech_count == 0:
        specificity_notes.append("‚ö†Ô∏è Peu de termes sp√©cifiques")

    # Mots de d√©tail
    detail_words = ['pr√©cis√©ment', 'exactement', 'sp√©cifiquement', 'notamment',
                   'precisely', 'exactly', 'specifically', 'including', 'such as']
    if any(d in prompt_lower for d in detail_words):
        specificity_score += 15
        specificity_notes.append("‚úÖ Indicateurs de pr√©cision")

    scores['specificity'] = min(100, max(0, specificity_score))
    details['specificity'] = specificity_notes

    # ==========================================================================
    # 4. STRUCTURE (0-100) - Organisation et formatage
    # ==========================================================================
    structure_score = 20  # Base faible
    structure_notes = []

    # Balises XML/Markdown
    if '<' in prompt and '>' in prompt:
        structure_score += 30
        structure_notes.append("‚úÖ Balises XML structurantes")

    # Headers Markdown
    if '##' in prompt or '###' in prompt:
        structure_score += 20
        structure_notes.append("‚úÖ Headers Markdown")

    # Listes
    if re.search(r'^\s*[-*]\s', prompt, re.MULTILINE) or re.search(r'^\s*\d+\.', prompt, re.MULTILINE):
        structure_score += 20
        structure_notes.append("‚úÖ Listes structur√©es")

    # Sections s√©par√©es
    if '\n\n' in prompt:
        structure_score += 10
        structure_notes.append("‚úÖ Paragraphes s√©par√©s")

    if structure_score <= 30:
        structure_notes.append("‚ùå Pas de structure claire")

    scores['structure'] = min(100, max(0, structure_score))
    details['structure'] = structure_notes

    # ==========================================================================
    # 5. FORMAT DE SORTIE (0-100) - Output attendu sp√©cifi√©
    # ==========================================================================
    output_score = 20  # Base faible
    output_notes = []

    format_markers = ['format:', 'output:', 'retourne', 'return', 'g√©n√®re un',
                     'produis', 'produce', 'en json', 'en markdown', 'as json',
                     '<output_format>', '<format>', 'format de sortie', 'output format']
    if any(f in prompt_lower for f in format_markers):
        output_score += 50
        output_notes.append("‚úÖ Format de sortie sp√©cifi√©")

    file_types = ['.json', '.md', '.py', '.js', '.ts', '.csv', '.xml', '.html', '.yaml']
    if any(ft in prompt_lower for ft in file_types):
        output_score += 20
        output_notes.append("‚úÖ Type de fichier sp√©cifi√©")

    if 'structure' in prompt_lower or 'template' in prompt_lower:
        output_score += 15
        output_notes.append("‚úÖ Structure de r√©ponse demand√©e")

    if output_score <= 30:
        output_notes.append("‚ùå Format de sortie non sp√©cifi√©")

    scores['output_format'] = min(100, max(0, output_score))
    details['output_format'] = output_notes

    # ==========================================================================
    # 6. CONTRAINTES (0-100) - Limites et r√®gles d√©finies
    # ==========================================================================
    constraints_score = 30  # Base
    constraints_notes = []

    constraint_words = ['ne pas', '√©vite', 'sans', 'maximum', 'minimum', 'limite',
                       'do not', 'avoid', 'without', 'max', 'min', 'limit', 'only',
                       'seulement', 'uniquement', 'doit', 'must', 'should not',
                       '<constraints>', '<requirements>', 'r√®gles:', 'rules:']
    constraint_count = sum(1 for c in constraint_words if c in prompt_lower)
    if constraint_count >= 2:
        constraints_score += 40
        constraints_notes.append(f"‚úÖ Contraintes explicites ({constraint_count})")
    elif constraint_count == 1:
        constraints_score += 20
        constraints_notes.append("‚úÖ Quelques contraintes")
    else:
        constraints_notes.append("‚ö†Ô∏è Pas de contraintes d√©finies")

    if re.search(r'\d+\s*(mots|words|lignes|lines|caract√®res|chars)', prompt_lower):
        constraints_score += 20
        constraints_notes.append("‚úÖ Limite de longueur sp√©cifi√©e")

    scores['constraints'] = min(100, max(0, constraints_score))
    details['constraints'] = constraints_notes

    # ==========================================================================
    # 7. EXEMPLES (0-100) - Pr√©sence d'exemples
    # ==========================================================================
    examples_score = 20  # Base faible
    examples_notes = []

    example_markers = ['exemple:', 'example:', 'par exemple', 'for example', 'e.g.',
                      'comme:', 'such as:', 'like:', 'voici un exemple', "here's an example",
                      '<example>', '<examples>', '```']
    if any(e in prompt_lower for e in example_markers):
        examples_score += 50
        examples_notes.append("‚úÖ Exemples fournis")

    if prompt.count('```') >= 2:
        examples_score += 30
        examples_notes.append("‚úÖ Blocs de code (exemples)")

    if examples_score <= 30:
        examples_notes.append("‚ö†Ô∏è Pas d'exemples fournis")

    scores['examples'] = min(100, max(0, examples_score))
    details['examples'] = examples_notes

    # ==========================================================================
    # SCORE GLOBAL POND√âR√â
    # ==========================================================================
    weights = {
        'clarity': 0.20,
        'context': 0.20,
        'specificity': 0.15,
        'structure': 0.15,
        'output_format': 0.12,
        'constraints': 0.10,
        'examples': 0.08
    }

    global_score = sum(scores[k] * weights[k] for k in weights)

    # Classification
    if global_score >= 80:
        grade, grade_label = 'A', 'üü¢ Excellent'
    elif global_score >= 65:
        grade, grade_label = 'B', 'üü° Bon'
    elif global_score >= 50:
        grade, grade_label = 'C', 'üü† Moyen'
    elif global_score >= 35:
        grade, grade_label = 'D', 'üî¥ Faible'
    else:
        grade, grade_label = 'F', '‚ö´ Insuffisant'

    return {
        'scores': scores,
        'details': details,
        'global_score': round(global_score, 1),
        'grade': grade,
        'grade_label': grade_label,
        'token_count': estimate_tokens(prompt),
        'char_count': len(prompt),
        'word_count': len(prompt.split())
    }


def compare_prompts(raw_prompt: str, formatted_prompt: str) -> str:
    """
    Compare le prompt brut et reformat√©, g√©n√®re un rapport d'am√©lioration
    bas√© sur l'analyse de la structure et du contenu.
    
    NOTE: Les m√©triques de qualit√© sont bas√©es sur des heuristiques (pr√©sence
    de balises XML, contexte, contraintes, etc.), pas sur des √©tudes empiriques.
    """
    raw_analysis = analyze_prompt_quality(raw_prompt)
    formatted_analysis = analyze_prompt_quality(formatted_prompt)

    score_diff = formatted_analysis['global_score'] - raw_analysis['global_score']
    diff_str = f"+{score_diff:.1f}" if score_diff >= 0 else f"{score_diff:.1f}"
    
    raw_score = raw_analysis['global_score']
    after_score = formatted_analysis['global_score']

    lines = []

    # ==========================================================================
    # SECTION 1: R√âSUM√â - Honn√™te sur ce qui est mesur√©
    # ==========================================================================
    lines.append("## üéØ Analyse du reformatage\n")

    if score_diff >= 25:
        value_verdict = "üöÄ **TRANSFORMATION MAJEURE**"
        verdict_note = "Le prompt a √©t√© consid√©rablement enrichi et structur√©."
    elif score_diff >= 15:
        value_verdict = "‚úÖ **AM√âLIORATION SIGNIFICATIVE**"
        verdict_note = "Structure et contexte nettement am√©lior√©s."
    elif score_diff >= 8:
        value_verdict = "üü° **AM√âLIORATION UTILE**"
        verdict_note = "Am√©liorations mod√©r√©es mais concr√®tes."
    elif score_diff >= 3:
        value_verdict = "‚û°Ô∏è **OPTIMISATION L√âG√àRE**"
        verdict_note = "Ajustements mineurs apport√©s."
    else:
        value_verdict = "üìã **PROMPT D√âJ√Ä OPTIMIS√â**"
        verdict_note = "Le prompt original √©tait d√©j√† bien formul√©."

    lines.append(f"### {value_verdict}\n")
    lines.append(f"*{verdict_note}*\n")

    # Tableau bas√© sur des MESURES R√âELLES du prompt
    len_ratio = formatted_analysis['char_count'] / max(raw_analysis['char_count'], 1)
    
    lines.append("| M√©trique | Avant | Apr√®s | Changement |")
    lines.append("|----------|-------|-------|------------|")
    lines.append(f"| Score qualit√© (heuristique) | {raw_score:.0f}% | {after_score:.0f}% | **{diff_str}%** |")
    lines.append(f"| Caract√®res | {raw_analysis['char_count']:,} | {formatted_analysis['char_count']:,} | **x{len_ratio:.1f}** |")
    lines.append(f"| Tokens estim√©s | ~{raw_analysis['token_count']} | ~{formatted_analysis['token_count']} | **x{formatted_analysis['token_count']/max(raw_analysis['token_count'],1):.1f}** |")
    
    # Structure XML d√©tect√©e
    has_xml_before = '<' in raw_prompt and '>' in raw_prompt
    has_xml_after = '<' in formatted_prompt and '>' in formatted_prompt
    xml_status = "‚úÖ Oui" if has_xml_after else "‚ùå Non"
    lines.append(f"| Structure XML | {'‚úÖ' if has_xml_before else '‚ùå'} | {xml_status} | {'üÜï Ajout√©e' if has_xml_after and not has_xml_before else '‚Äî'} |")

    lines.append("\n> ‚ö†Ô∏è *Le \"score qualit√©\" est une heuristique bas√©e sur la pr√©sence de structure,")
    lines.append("> contexte, contraintes, etc. Ce n'est pas une mesure de performance r√©elle.*")

    # ==========================================================================
    # SECTION 2: CE QUI A √âT√â AM√âLIOR√â (factuel)
    # ==========================================================================
    lines.append("\n### üîç Am√©liorations d√©tect√©es\n")

    concrete_improvements = []

    if '<' in formatted_prompt and '>' in formatted_prompt and '<' not in raw_prompt:
        concrete_improvements.append("üìê **Structure XML ajout√©e** : Balises `<task>`, `<context>`, `<instructions>`")

    if formatted_analysis['scores']['context'] > raw_analysis['scores']['context'] + 15:
        concrete_improvements.append("üìñ **Contexte enrichi** : Section contexte d√©tect√©e/am√©lior√©e")

    if formatted_analysis['scores']['specificity'] > raw_analysis['scores']['specificity'] + 15:
        concrete_improvements.append("üîç **Sp√©cificit√© augment√©e** : Plus de termes techniques d√©tect√©s")

    if formatted_analysis['scores']['output_format'] > raw_analysis['scores']['output_format'] + 15:
        concrete_improvements.append("üì§ **Format de sortie** : Instructions de format ajout√©es")

    if formatted_analysis['scores']['constraints'] > raw_analysis['scores']['constraints'] + 15:
        concrete_improvements.append("‚ö†Ô∏è **Contraintes** : Limites et r√®gles explicites ajout√©es")

    if formatted_analysis['scores']['examples'] > raw_analysis['scores']['examples'] + 15:
        concrete_improvements.append("üí° **Exemples** : Cas concrets ajout√©s")

    if formatted_analysis['scores']['clarity'] > raw_analysis['scores']['clarity'] + 15:
        concrete_improvements.append("üéØ **Clart√©** : Verbes d'action plus directs")

    if len_ratio > 2:
        concrete_improvements.append(f"üìù **Enrichissement** : Prompt {len_ratio:.1f}x plus long")

    if concrete_improvements:
        for imp in concrete_improvements:
            lines.append(f"- {imp}")
    else:
        lines.append("- Aucune am√©lioration majeure d√©tect√©e (prompt original d√©j√† structur√©)")

    # ==========================================================================
    # SECTION 3: SCORE DE STRUCTURE (heuristique)
    # ==========================================================================
    lines.append("\n### üìä Score de Structure (heuristique)\n")
    
    lines.append("> ‚ö†Ô∏è *Ce score mesure la STRUCTURE du prompt (balises, contexte, contraintes), "
                "pas la qualit√© des r√©ponses. L'impact r√©el d√©pend du mod√®le et de la t√¢che.*\n")

    before_bar = "‚ñà" * int(raw_analysis['global_score'] / 10) + "‚ñë" * (10 - int(raw_analysis['global_score'] / 10))
    after_bar = "‚ñà" * int(formatted_analysis['global_score'] / 10) + "‚ñë" * (10 - int(formatted_analysis['global_score'] / 10))

    lines.append(f"**Avant:** {raw_analysis['grade_label']} `{before_bar}` {raw_analysis['global_score']}%")
    lines.append(f"**Apr√®s:** {formatted_analysis['grade_label']} `{after_bar}` {formatted_analysis['global_score']}%")
    lines.append(f"**Gain structurel:** {diff_str}%\n")

    # Tableau d√©taill√© par crit√®re
    lines.append("| Crit√®re | Avant | Apr√®s | Œî |")
    lines.append("|---------|:-----:|:-----:|:-:|")

    criteria_labels = {
        'clarity': 'üéØ Clart√©',
        'context': 'üìñ Contexte',
        'specificity': 'üîç Sp√©cificit√©',
        'structure': 'üìê Structure',
        'output_format': 'üì§ Format sortie',
        'constraints': '‚ö†Ô∏è Contraintes',
        'examples': 'üí° Exemples'
    }

    for key, label in criteria_labels.items():
        before = raw_analysis['scores'][key]
        after = formatted_analysis['scores'][key]
        diff = after - before

        if diff > 15:
            diff_icon = f"üü¢ **+{diff}**"
        elif diff > 5:
            diff_icon = f"üü° +{diff}"
        elif diff > 0:
            diff_icon = f"‚¨ÜÔ∏è +{diff}"
        elif diff == 0:
            diff_icon = "‚û°Ô∏è 0"
        else:
            diff_icon = f"üî¥ {diff}"

        lines.append(f"| {label} | {before}% | {after}% | {diff_icon} |")

    # ==========================================================================
    # SECTION 4: M√âTRIQUES MESUR√âES (100% v√©rifiables)
    # ==========================================================================
    lines.append("\n### üìè M√©triques Mesur√©es\n")
    lines.append("*Ces valeurs sont calcul√©es directement, sans estimation.*\n")

    lines.append("| M√©trique | Avant | Apr√®s | Ratio |")
    lines.append("|----------|------:|------:|------:|")
    lines.append(f"| Caract√®res | {raw_analysis['char_count']:,} | {formatted_analysis['char_count']:,} | x{len_ratio:.1f} |")
    lines.append(f"| Mots | {raw_analysis['word_count']} | {formatted_analysis['word_count']} | x{formatted_analysis['word_count']/max(raw_analysis['word_count'],1):.1f} |")
    lines.append(f"| Tokens (est.) | ~{raw_analysis['token_count']} | ~{formatted_analysis['token_count']} | x{formatted_analysis['token_count']/max(raw_analysis['token_count'],1):.1f} |")
    
    # Co√ªt tokens suppl√©mentaires
    extra_tokens = formatted_analysis['token_count'] - raw_analysis['token_count']
    if extra_tokens > 0:
        # Estimation co√ªt avec Claude Sonnet ($3/$15 per M)
        extra_cost = extra_tokens * 0.000003
        lines.append(f"\n**Tokens suppl√©mentaires:** +{extra_tokens} (~${extra_cost:.6f} par requ√™te avec Sonnet)")

    # ==========================================================================
    # SECTION 5: IMPACT ATTENDU (bas√© sur la recherche)
    # ==========================================================================
    lines.append("\n---\n### üî¨ Impact Attendu (bas√© sur la recherche)\n")
    
    research_impacts = []
    
    # XML ajout√©
    has_xml_before = '<' in raw_prompt and '>' in raw_prompt
    has_xml_after = '<' in formatted_prompt and '>' in formatted_prompt
    if not has_xml_before and has_xml_after:
        research_impacts.append({
            "feature": "üìê Structure XML ajout√©e",
            "impact": "‚ÜîÔ∏è Variable",
            "range": "-5% √† +40%",
            "source": "Voyce et al. 2024 (arXiv:2411.10541)",
            "note": "GPT-3.5 varie jusqu'√† 40%, GPT-4 ~5%. Claude optimis√© pour XML."
        })
    
    # √âtapes num√©rot√©es
    has_steps_before = len(re.findall(r'^\s*\d+[\.\)]\s', raw_prompt, re.MULTILINE)) >= 2
    has_steps_after = len(re.findall(r'^\s*\d+[\.\)]\s', formatted_prompt, re.MULTILINE)) >= 2
    if not has_steps_before and has_steps_after:
        research_impacts.append({
            "feature": "üî¢ √âtapes num√©rot√©es",
            "impact": "üìà Positif",
            "range": "+50% √† +87%",
            "source": "Latitude 2025",
            "note": "Meilleure compliance aux instructions."
        })
    
    # Exemples (few-shot)
    example_markers = ['example:', 'exemple:', '<example>', 'input:', 'output:']
    has_examples_before = any(m in raw_prompt.lower() for m in example_markers)
    has_examples_after = any(m in formatted_prompt.lower() for m in example_markers)
    if not has_examples_before and has_examples_after:
        research_impacts.append({
            "feature": "üí° Exemples (few-shot)",
            "impact": "üìà Positif",
            "range": "+7% √† +12%",
            "source": "GPT-3 paper, Analytics Vidhya 2025",
            "note": "Plateau apr√®s 2-8 exemples."
        })
    
    # Chain-of-Thought
    cot_markers = ['step by step', '√©tape par √©tape', 'think through']
    has_cot_before = any(m in raw_prompt.lower() for m in cot_markers)
    has_cot_after = any(m in formatted_prompt.lower() for m in cot_markers)
    if not has_cot_before and has_cot_after:
        research_impacts.append({
            "feature": "üß† Chain-of-Thought",
            "impact": "‚ÜîÔ∏è Variable",
            "range": "-36% √† +87%",
            "source": "Wei et al. 2022, Wharton 2025",
            "note": "‚ö†Ô∏è Excellent pour math/raisonnement, peut nuire sur d'autres t√¢ches. +300% temps."
        })
    
    if research_impacts:
        lines.append("| Caract√©ristique | Impact | Plage | Source |")
        lines.append("|-----------------|--------|-------|--------|")
        for ri in research_impacts:
            lines.append(f"| {ri['feature']} | {ri['impact']} | {ri['range']} | {ri['source']} |")
        
        lines.append("\n**Notes des √©tudes:**")
        for ri in research_impacts:
            lines.append(f"- *{ri['feature']}*: {ri['note']}")
    else:
        lines.append("*Pas de changement majeur d√©tect√© par rapport aux √©tudes de r√©f√©rence.*")

    # ==========================================================================
    # SECTION 6: CE QU'ON NE PEUT PAS PR√âDIRE
    # ==========================================================================
    lines.append("\n---\n### ‚ö†Ô∏è Ce qu'on ne peut PAS pr√©dire\n")
    lines.append("""
Sans tests A/B r√©els sur votre cas d'usage sp√©cifique, il est **impossible** de pr√©dire:

- ‚ùå Le % exact de "r√©ponses utiles du premier essai"
- ‚ùå Le nombre d'it√©rations n√©cessaires
- ‚ùå Le temps √©conomis√© par t√¢che
- ‚ùå Le "risque de mauvaise interpr√©tation"

*Ces m√©triques d√©pendent du mod√®le, de la t√¢che, et du contexte. Testez les deux versions!*
""")

    # ==========================================================================
    # SECTION 7: BONNES PRATIQUES
    # ==========================================================================
    lines.append("\n---\n### üìú Bonnes Pratiques\n")
    lines.append("*Recommandations des principaux fournisseurs:*\n")
    
    lines.append("| Fournisseur | Recommandation |")
    lines.append("|-------------|----------------|")
    lines.append("| **Anthropic** | Utiliser des balises XML pour structurer les prompts |")
    lines.append("| **OpenAI** | S√©parer clairement instructions et contexte |")
    lines.append("| **Google** | Fournir des exemples et du contexte d√©taill√© |")
    
    lines.append("\n> üí° **Consensus:** La structure claire am√©liore les r√©sultats des LLM.")

    # ==========================================================================
    # SECTION 8: VERDICT HONN√äTE
    # ==========================================================================
    lines.append("\n---\n## üéØ Verdict\n")

    if score_diff >= 20 and len(research_impacts) >= 2:
        lines.append("### ‚úÖ Reformatage recommand√©\n")
        lines.append(f"Le prompt reformat√© ajoute {len(research_impacts)} caract√©ristique(s) "
                    f"associ√©e(s) √† des am√©liorations dans la recherche publi√©e.\n")
        lines.append(f"**Score structurel:** +{score_diff:.0f}% | **Expansion:** x{len_ratio:.1f}\n")
        lines.append("‚ö†Ô∏è *L'impact r√©el d√©pend de votre mod√®le et t√¢che. Les √©tudes montrent des r√©sultats variables.*")
    elif score_diff >= 10:
        lines.append("### üü° Reformatage potentiellement utile\n")
        lines.append(f"Am√©lioration structurelle de **+{score_diff:.0f}%**, mais l'impact r√©el d√©pend du contexte.")
        lines.append("\n*Testez les deux versions pour valider.*")
    elif len_ratio > 3.0:
        lines.append("### ‚ö†Ô∏è Attention: expansion importante\n")
        lines.append(f"Le prompt est **{len_ratio:.1f}x plus long** (+{(len_ratio-1)*100:.0f}% de tokens).\n")
        lines.append("Cette augmentation de co√ªt n'est justifi√©e que si la qualit√© de sortie s'am√©liore significativement.")
        lines.append("\n*Testez pour v√©rifier si l'expansion apporte de la valeur.*")
    else:
        lines.append("### ‚û°Ô∏è Changements mod√©r√©s\n")
        lines.append("Le reformatage apporte des ajustements mineurs. L'impact d√©pendra de votre cas d'usage.")

    return "\n".join(lines)


def detect_task_type(prompt: str) -> str:
    """D√©tecte le type de t√¢che √† partir du prompt."""
    prompt_lower = prompt.lower()

    code_keywords = ['code', 'fonction', 'function', 'api', 'endpoint', 'bug', 'debug',
                     'refactor', 'test', 'class', 'method', 'variable', 'import',
                     'database', 'sql', 'query', 'script', 'algorithm', 'implementation',
                     'module', 'library', 'framework', 'backend', 'frontend', 'crud',
                     'route', 'controller', 'model', 'schema', 'migration', 'deploy',
                     'docker', 'git', 'commit', 'branch', 'merge', 'pull request']

    analysis_keywords = ['analyse', 'analyze', 'research', 'study', 'compare', 'evaluate',
                        'review', 'audit', 'report', 'summary', 'synthesize', 'document',
                        'r√©sum√©', 'synth√®se', 'donn√©es', 'data', 'statistics', 'metrics',
                        'benchmark', 'performance', 'optimize', 'am√©liorer', 'improve']

    creative_keywords = ['write', 'create', 'design', 'imagine', 'story', 'article',
                        'blog', 'content', 'creative', 'idea', 'concept', 'brand',
                        '√©cris', 'r√©dige', 'histoire', 'r√©cit', 'po√®me', 'slogan',
                        'marketing', 'publicit√©', 'campagne', 'narratif', 'fiction']

    chat_keywords = ['explain', 'help', 'what is', 'how to', 'question', 'answer',
                    'clarify', 'describe', 'tell me', 'explique', 'aide', 'comment',
                    'pourquoi', 'quest-ce', 'd√©finition', 'definition', 'meaning',
                    'understand', 'comprendre', 'learn', 'apprendre', 'tutorial']

    code_score = sum(1 for k in code_keywords if k in prompt_lower)
    analysis_score = sum(1 for k in analysis_keywords if k in prompt_lower)
    creative_score = sum(1 for k in creative_keywords if k in prompt_lower)
    chat_score = sum(1 for k in chat_keywords if k in prompt_lower)

    if '<context>' in prompt_lower or '<task>' in prompt_lower:
        code_score += 2
    if '```' in prompt_lower:
        code_score += 3
    if 'requirements' in prompt_lower or 'specifications' in prompt_lower:
        code_score += 1

    scores = {
        'code': code_score,
        'analysis': analysis_score,
        'creative': creative_score,
        'chat': chat_score
    }

    max_type = max(scores, key=scores.get)
    if scores[max_type] == 0:
        return 'general'
    return max_type


def detect_domain(prompt: str) -> str:
    """D√©tecte le domaine du prompt pour une recommandation pr√©cise."""
    prompt_lower = prompt.lower()

    domains = {
        # === DOMAINES TECHNIQUES ===
        'code': ['code', 'function', 'api', 'endpoint', 'bug', 'debug', 'refactor',
                'test', 'class', 'method', 'import', 'database', 'sql', 'script',
                'algorithm', 'backend', 'frontend', 'deploy', 'docker', 'git',
                'python', 'javascript', 'typescript', 'react', 'fastapi', 'node',
                'variable', 'compiler', 'runtime', 'library', 'framework', 'component'],

        'data': ['data', 'analytics', 'dashboard', 'report', 'metrics', 'kpi',
                'visualization', 'statistics', 'dataset', 'csv', 'excel', 'tableau',
                'donn√©es', 'rapport', 'statistiques', 'graphique', 'bigquery', 'snowflake',
                'sql query', 'etl', 'pipeline', 'warehouse', 'dbt', 'looker', 'powerbi'],

        # === DOMAINES M√âTIERS (NOUVEAUX) ===
        'seo': ['seo', 'keyword', 'backlink', 'serp', 'ranking', 'organic', 'meta description',
               'title tag', 'alt text', 'sitemap', 'robots.txt', 'canonical', 'indexation',
               'r√©f√©rencement', 'mot-cl√©', 'mots-cl√©s', 'position google', 'search console',
               'ahrefs', 'semrush', 'moz', 'domain authority', 'page authority', 'crawl',
               'longue tra√Æne', 'featured snippet', 'core web vitals', 'lighthouse'],

        'marketing': ['marketing', 'campaign', 'ads', 'advertising', 'funnel', 'conversion',
                     'lead generation', 'cac', 'ltv', 'roas', 'ctr', 'cpc', 'cpm', 'roi',
                     'persona', 'target audience', 'ab test', 'landing page', 'copywriting',
                     'email marketing', 'newsletter', 'automation', 'hubspot', 'mailchimp',
                     'google ads', 'facebook ads', 'meta ads', 'linkedin ads', 'retargeting',
                     'campagne', 'publicit√©', 'acquisition', 'growth', 'branding', 'awareness'],

        'hr': ['rh', 'hr', 'recrutement', 'recruitment', 'hiring', 'candidate', 'candidat',
              'job description', 'fiche de poste', 'onboarding', 'offboarding', 'talent',
              'entretien', 'interview', 'cv', 'resume', 'sourcing', 'linkedin recruiter',
              'salaire', 'salary', 'compensation', 'benefits', 'avantages', 'culture',
              'turnover', 'retention', 'performance review', 'feedback', 'formation',
              'training', 'd√©veloppement', 'carri√®re', 'mobilit√©', 'ats', 'lever', 'greenhouse'],

        'sales': ['sales', 'vente', 'commercial', 'prospect', 'prospection', 'pipeline',
                 'deal', 'closing', 'n√©gociation', 'negotiation', 'objection', 'pitch',
                 'crm', 'salesforce', 'hubspot', 'pipedrive', 'cold email', 'cold call',
                 'discovery call', 'demo', 'proposal', 'devis', 'pricing', 'discount',
                 'quota', 'forecast', 'revenue', 'arr', 'mrr', 'churn', 'upsell', 'cross-sell',
                 'account executive', 'sdr', 'bdr', 'account manager', 'client', 'customer'],

        'product': ['product', 'produit', 'roadmap', 'backlog', 'user story', 'epic', 'sprint',
                   'agile', 'scrum', 'kanban', 'jira', 'linear', 'notion', 'productboard',
                   'prd', 'spec', 'specification', 'feature', 'mvp', 'pmf', 'product market fit',
                   'user research', 'discovery', 'a/b test', 'north star', 'okr', 'kpi',
                   'prioritization', 'rice', 'ice', 'moscow', 'stakeholder', 'release'],

        'support': ['support client', 'customer service', 'helpdesk', 'ticket support', 'zendesk', 'intercom',
                   'freshdesk', 'crisp', 'csat', 'nps support', 'satisfaction client', 'complaint', 'plainte',
                   'resolution ticket', 'escalation', 'sla support', 'response time', 'first contact resolution',
                   'knowledge base', 'faq', 'help center', 'chatbot support', 'live chat', 'service client',
                   'customer success manager', 'onboarding client', 'churn prevention', 'client m√©content',
                   'r√©pondre au ticket', 'probl√®me client', 'r√©clamation', 'assistance'],

        # === DOMAINES SP√âCIALIS√âS ===
        'legal': ['legal', 'law', 'contract', 'clause', 'attorney', 'lawyer', 'court',
                  'juridique', 'contrat', 'avocat', 'tribunal', 'loi', 'r√®glement',
                  'compliance', 'regulation', 'litigation', 'lawsuit', 'patent', 'rgpd',
                  'gdpr', 'cnil', 'dpo', 'nda', 'cgv', 'cgu', 'propri√©t√© intellectuelle'],

        'medical': ['medical', 'health', 'doctor', 'patient', 'diagnosis', 'treatment',
                   'symptom', 'disease', 'medication', 'clinical', 'hospital',
                   'm√©dical', 'sant√©', 'm√©decin', 'diagnostic', 'traitement', 'maladie',
                   'diab√®te', 'hypertension', 'fatigue', 'douleur', 'fi√®vre'],

        'finance': ['financial', 'investment', 'stock', 'trading', 'portfolio', 'revenue',
                   'profit', 'accounting', 'audit', 'tax', 'budget', 'forecast',
                   'financier', 'investissement', 'bourse', 'comptabilit√©', 'imp√¥t'],

        'creative': ['write', 'story', 'article', 'blog', 'creative', 'poem', 'fiction',
                    'narrative', 'script', 'screenplay', 'novel', 'content',
                    '√©cris', 'histoire', 'r√©cit', 'cr√©atif', 'r√©daction'],

        'research': ['research', 'study', 'analyze', 'paper', 'thesis', 'literature',
                    'scientific', 'academic', 'peer-review', 'hypothesis', 'experiment',
                    'recherche', '√©tude', 'analyse', 'scientifique', 'acad√©mique'],

        'math': ['math', 'equation', 'calcul', 'formula', 'proof', 'theorem',
                'algebra', 'geometry', 'calculus', 'probability', 'statistics',
                'math√©matique', '√©quation', 'formule', 'preuve', 'th√©or√®me'],

        'image': ['image', 'picture', 'photo', 'illustration', 'generate image', 'draw',
                 'visual', 'artwork', 'design', 'logo', 'banner', 'poster', 'graphic',
                 'midjourney', 'dall-e', 'dalle', 'stable diffusion', 'flux',
                 'g√©n√®re une image', 'dessine', 'cr√©e une image', 'illustre'],

        'document': ['document', 'pdf', 'file', 'read', 'extract', 'summarize document',
                    'analyze document', 'ocr', 'scan', 'attachment', 'upload',
                    'fichier', 'lire', 'extraire', 'r√©sumer le document', 'pi√®ce jointe'],
    }

    scores = {}
    for domain, keywords in domains.items():
        scores[domain] = sum(1 for k in keywords if k in prompt_lower)

    # Protection des domaines sp√©cialis√©s (ne pas √©craser par 'code' si match fort)
    protected_domains = ['legal', 'medical', 'finance', 'math', 'image', 'document', 
                        'creative', 'seo', 'marketing', 'hr', 'sales', 'product', 'support']
    max_protected = max(protected_domains, key=lambda d: scores.get(d, 0))
    protected_score = scores.get(max_protected, 0)

    if protected_score < 1:
        prompt_for_code_check = prompt_lower
        for xml_tag in ['<context>', '</context>', '<task>', '</task>', '<requirements>',
                        '</requirements>', '<output_format>', '</output_format>']:
            prompt_for_code_check = prompt_for_code_check.replace(xml_tag, '')

        real_code_patterns = ['```', 'def ', 'function ', 'class ', 'import ', 'const ', 'let ', 'var ']
        has_real_code = any(pattern in prompt_for_code_check for pattern in real_code_patterns)

        if has_real_code:
            scores['code'] = scores.get('code', 0) + 3

    max_domain = max(scores, key=scores.get)
    if scores[max_domain] == 0:
        return 'general'
    return max_domain
