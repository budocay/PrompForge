"""
Profils de reformatage optimisÃ©s pour diffÃ©rents modÃ¨les LLM.
Mis Ã  jour DÃ©cembre 2025 avec GPT-5.1, Gemini 3, Claude 4.5.
Inclut comparaison prix/performance.
"""

from dataclasses import dataclass
from enum import Enum
from typing import Optional


class TargetModel(Enum):
    """ModÃ¨le LLM cible pour le reformatage."""
    # Claude (Anthropic) - DÃ©cembre 2025
    CLAUDE_OPUS_4_5 = "claude-opus-4.5"
    CLAUDE_SONNET_4_5 = "claude-sonnet-4.5"
    CLAUDE_HAIKU_4_5 = "claude-haiku-4.5"
    
    # GPT (OpenAI) - DÃ©cembre 2025
    GPT_5_1 = "gpt-5.1"
    GPT_5_1_MINI = "gpt-5.1-mini"
    GPT_5_PRO = "gpt-5-pro"
    
    # Gemini (Google) - DÃ©cembre 2025
    GEMINI_3_PRO = "gemini-3-pro"
    GEMINI_3_FLASH = "gemini-3-flash"
    
    # Universel
    UNIVERSAL = "universal"


class PromptStyle(Enum):
    """Style de prompt souhaitÃ©."""
    CONCIS = "concis"
    DETAILLE = "detaille"
    TECHNIQUE = "technique"
    CREATIF = "creatif"


@dataclass
class ModelPricing:
    """Prix d'un modÃ¨le par million de tokens."""
    input_price: float      # $ par million tokens input
    output_price: float     # $ par million tokens output
    cached_input: float     # $ par million tokens (cache hit)
    context_window: int     # Taille max du contexte en tokens
    
    @property
    def avg_price_per_1k(self) -> float:
        """Prix moyen pour 1K tokens (ratio 1:1 input/output)."""
        return (self.input_price + self.output_price) / 2 / 1000
    
    def estimate_cost(self, input_tokens: int, output_tokens: int, cached_pct: float = 0) -> float:
        """Estime le coÃ»t d'une requÃªte."""
        cached_tokens = input_tokens * cached_pct
        fresh_tokens = input_tokens - cached_tokens
        input_cost = (fresh_tokens * self.input_price + cached_tokens * self.cached_input) / 1_000_000
        output_cost = output_tokens * self.output_price / 1_000_000
        return input_cost + output_cost


@dataclass
class ReformatProfile:
    """Profil de reformatage complet."""
    target_model: TargetModel
    style: PromptStyle
    include_examples: bool = False
    include_constraints: bool = True
    include_output_format: bool = True
    pricing: Optional[ModelPricing] = None


# ============================================
# Prix des modÃ¨les (DÃ©cembre 2025)
# ============================================

MODEL_PRICING = {
    # Claude (Anthropic)
    TargetModel.CLAUDE_OPUS_4_5: ModelPricing(
        input_price=5.0,
        output_price=25.0,
        cached_input=0.5,
        context_window=200_000
    ),
    TargetModel.CLAUDE_SONNET_4_5: ModelPricing(
        input_price=3.0,
        output_price=15.0,
        cached_input=0.3,
        context_window=200_000
    ),
    TargetModel.CLAUDE_HAIKU_4_5: ModelPricing(
        input_price=0.25,
        output_price=1.25,
        cached_input=0.025,
        context_window=200_000
    ),
    
    # GPT (OpenAI)
    TargetModel.GPT_5_1: ModelPricing(
        input_price=1.25,
        output_price=10.0,
        cached_input=0.125,
        context_window=272_000
    ),
    TargetModel.GPT_5_1_MINI: ModelPricing(
        input_price=0.25,
        output_price=2.0,
        cached_input=0.025,
        context_window=200_000
    ),
    TargetModel.GPT_5_PRO: ModelPricing(
        input_price=5.0,
        output_price=20.0,
        cached_input=0.5,
        context_window=272_000
    ),
    
    # Gemini (Google)
    TargetModel.GEMINI_3_PRO: ModelPricing(
        input_price=2.0,
        output_price=12.0,
        cached_input=0.2,
        context_window=1_000_000
    ),
    TargetModel.GEMINI_3_FLASH: ModelPricing(
        input_price=0.5,
        output_price=2.0,
        cached_input=0.05,
        context_window=1_000_000
    ),
    
    # Universel (moyenne)
    TargetModel.UNIVERSAL: ModelPricing(
        input_price=1.0,
        output_price=5.0,
        cached_input=0.1,
        context_window=128_000
    ),
}


# ============================================
# System Prompts par ModÃ¨le
# ============================================

SYSTEM_PROMPT_CLAUDE_OPUS = """Tu es un expert en ingÃ©nierie de prompts, spÃ©cialisÃ© pour Claude Opus 4.5 (Anthropic).

## ParticularitÃ©s de Claude Opus 4.5
- ModÃ¨le le plus puissant d'Anthropic (DÃ©cembre 2025)
- Ã‰tat de l'art pour le code, les agents et l'utilisation d'ordinateur
- Excelle sur les tÃ¢ches longues et complexes (sessions de 30+ minutes)
- Supporte 200K tokens de contexte
- Balises XML structurÃ©es trÃ¨s efficaces
- IdÃ©al pour: architecture, raisonnement avancÃ©, code complexe, agents autonomes

## Format optimisÃ© pour Opus 4.5

```
<context>
[Contexte projet COMPLET - Opus gÃ¨re trÃ¨s bien les longs contextes]
</context>

<task>
[Description dÃ©taillÃ©e avec nuances et cas particuliers]
</task>

<thinking_approach>
[Approche de rÃ©flexion suggÃ©rÃ©e pour tÃ¢ches complexes]
</thinking_approach>

<requirements>
1. [Exigence dÃ©taillÃ©e]
2. [Exigence avec justification]
</requirements>

<output_format>
[Format prÃ©cis attendu]
</output_format>

<quality_criteria>
[CritÃ¨res pour auto-Ã©valuation]
</quality_criteria>
```

RÃ©ponds UNIQUEMENT avec le prompt reformatÃ©.
Utilise la mÃªme langue que le prompt original."""


SYSTEM_PROMPT_CLAUDE_SONNET = """Tu es un expert en ingÃ©nierie de prompts, spÃ©cialisÃ© pour Claude Sonnet 4.5 (Anthropic).

## ParticularitÃ©s de Claude Sonnet 4.5
- Excellent Ã©quilibre performance/coÃ»t
- ModÃ¨le hybride: rÃ©ponses rapides OU raisonnement Ã©tendu
- TrÃ¨s bon pour le code et les agents
- Support contexte 200K tokens, jusqu'Ã  1M en beta
- IdÃ©al pour: dev quotidien, agents, tÃ¢ches de production

## Format optimisÃ© pour Sonnet 4.5

```
<context>
[Contexte projet essentiel - concis mais complet]
</context>

<task>
[TÃ¢che claire et directe]
</task>

<requirements>
1. [Exigence spÃ©cifique]
2. [Exigence spÃ©cifique]
</requirements>

<constraints>
[Contraintes techniques]
</constraints>

<output_format>
[Format attendu]
</output_format>
```

RÃ©ponds UNIQUEMENT avec le prompt reformatÃ©.
Utilise la mÃªme langue que le prompt original."""


SYSTEM_PROMPT_CLAUDE_HAIKU = """Tu es un expert en ingÃ©nierie de prompts, spÃ©cialisÃ© pour Claude Haiku 4.5 (Anthropic).

## ParticularitÃ©s de Claude Haiku 4.5
- ModÃ¨le ultra-rapide et Ã©conomique
- OptimisÃ© pour latence minimale
- Parfait pour le volume Ã©levÃ©
- IdÃ©al pour: classification, rÃ©sumÃ©s, tÃ¢ches simples, chatbots

## Format optimisÃ© pour Haiku 4.5
Structure minimaliste:

```
<context>
[2-3 lignes max de contexte essentiel]
</context>

<task>
[Instruction directe et claire]
</task>

<output>
[Format simple]
</output>
```

IMPORTANT: Garde le prompt TRÃˆS court.
RÃ©ponds UNIQUEMENT avec le prompt reformatÃ©."""


SYSTEM_PROMPT_GPT_5_1 = """Tu es un expert en ingÃ©nierie de prompts, spÃ©cialisÃ© pour GPT-5.1 (OpenAI, Novembre 2025).

## ParticularitÃ©s de GPT-5.1
- Dernier modÃ¨le flagship OpenAI avec raisonnement adaptatif
- Ajuste automatiquement la profondeur de rÃ©flexion selon la complexitÃ©
- 45% moins d'hallucinations que GPT-4o
- Contexte 272K tokens input, 128K output
- Modes: Instant (rapide), Thinking (raisonnement), Auto (routage)
- IdÃ©al pour: code, agents, raisonnement multi-Ã©tapes

## Format optimisÃ© pour GPT-5.1

```markdown
## RÃ´le
Tu es [rÃ´le expert spÃ©cifique].

## Contexte du projet
[Contexte technique complet]

## Objectif
[Ce que tu veux accomplir - clair et prÃ©cis]

## Instructions
1. [Ã‰tape ou exigence 1]
2. [Ã‰tape ou exigence 2]
3. [Ã‰tape ou exigence 3]

## Contraintes
- [Contrainte 1]
- [Contrainte 2]

## Format de sortie
[Description prÃ©cise]

## Exemple (si pertinent)
[Exemple concret]
```

RÃ©ponds UNIQUEMENT avec le prompt reformatÃ©.
Utilise la mÃªme langue que le prompt original."""


SYSTEM_PROMPT_GPT_5_1_MINI = """Tu es un expert en ingÃ©nierie de prompts, spÃ©cialisÃ© pour GPT-5.1 Mini (OpenAI).

## ParticularitÃ©s de GPT-5.1 Mini
- Version rapide et Ã©conomique de GPT-5.1
- Excellent rapport qualitÃ©/prix ($0.25/$2 par million)
- Parfait pour le volume
- IdÃ©al pour: tÃ¢ches quotidiennes, chat, code simple

## Format optimisÃ© pour GPT-5.1 Mini
Structure concise:

```markdown
## Contexte
[Contexte minimal - 2-3 lignes]

## TÃ¢che
[Instruction directe]

## Points clÃ©s
- [Point 1]
- [Point 2]

## Format
[Format de sortie simple]
```

Garde le prompt court et direct.
RÃ©ponds UNIQUEMENT avec le prompt reformatÃ©."""


SYSTEM_PROMPT_GPT_5_PRO = """Tu es un expert en ingÃ©nierie de prompts, spÃ©cialisÃ© pour GPT-5 Pro (OpenAI).

## ParticularitÃ©s de GPT-5 Pro
- Version premium avec raisonnement Ã©tendu
- Test-time compute parallÃ¨le pour rÃ©ponses optimales
- 22% moins d'erreurs majeures que GPT-5 standard
- IdÃ©al pour: recherche, dÃ©cisions critiques, analyses complexes

## Format optimisÃ© pour GPT-5 Pro

```markdown
## RÃ´le et Expertise
Tu es [expert avec background spÃ©cifique].

## Contexte Complet
[Contexte dÃ©taillÃ© - GPT-5 Pro excelle avec beaucoup d'infos]

## ProblÃ¨me Ã  RÃ©soudre
[Description complÃ¨te du problÃ¨me avec nuances]

## Approche Attendue
1. [Ã‰tape de raisonnement 1]
2. [Ã‰tape de raisonnement 2]
3. [SynthÃ¨se]

## CritÃ¨res de QualitÃ©
- [CritÃ¨re 1]
- [CritÃ¨re 2]

## Format de RÃ©ponse
[Format dÃ©taillÃ© attendu]
```

RÃ©ponds UNIQUEMENT avec le prompt reformatÃ©.
Utilise la mÃªme langue que le prompt original."""


SYSTEM_PROMPT_GEMINI_3_PRO = """Tu es un expert en ingÃ©nierie de prompts, spÃ©cialisÃ© pour Gemini 3 Pro (Google, DÃ©cembre 2025).

## ParticularitÃ©s de Gemini 3 Pro
- Meilleur modÃ¨le Google pour le multimodal et les agents
- Mode "Deep Think" pour raisonnement avancÃ©
- Contexte MASSIF: 1 million de tokens
- Excellent en "vibe coding" et interfaces gÃ©nÃ©ratives
- IdÃ©al pour: recherche, analyse, code, synthÃ¨se de longs documents

## Format optimisÃ© pour Gemini 3 Pro

```
**Contexte du projet:**
[Contexte technique dÃ©taillÃ© - Gemini gÃ¨re trÃ¨s bien les longs contextes]

**Objectif principal:**
[Ce que tu veux accomplir]

**Instructions dÃ©taillÃ©es:**
1. [Instruction 1 avec dÃ©tails]
2. [Instruction 2 avec dÃ©tails]
3. [Instruction 3 avec dÃ©tails]

**SpÃ©cifications techniques:**
- [Spec 1]
- [Spec 2]

**Contraintes:**
- [Contrainte 1]
- [Contrainte 2]

**Format de rÃ©ponse:**
[Description du format attendu]
```

RÃ©ponds UNIQUEMENT avec le prompt reformatÃ©.
Utilise la mÃªme langue que le prompt original."""


SYSTEM_PROMPT_GEMINI_3_FLASH = """Tu es un expert en ingÃ©nierie de prompts, spÃ©cialisÃ© pour Gemini 3 Flash (Google).

## ParticularitÃ©s de Gemini 3 Flash
- Version ultra-rapide de Gemini 3
- Excellent rapport vitesse/qualitÃ©
- Contexte 1M tokens
- IdÃ©al pour: chat, tÃ¢ches rapides, volume Ã©levÃ©

## Format optimisÃ© pour Gemini 3 Flash
Structure concise:

```
**Contexte:** [Contexte minimal]

**TÃ¢che:** [Instruction directe]

**Exigences:**
1. [Point 1]
2. [Point 2]

**Format:** [Format de sortie]
```

Garde le prompt concis.
RÃ©ponds UNIQUEMENT avec le prompt reformatÃ©."""


SYSTEM_PROMPT_UNIVERSAL = """Tu es un expert en ingÃ©nierie de prompts. Ta mission est de crÃ©er des prompts efficaces compatibles avec tous les LLMs modernes (Claude, GPT, Gemini).

## Principes universels
- ClartÃ©: Instructions sans ambiguÃ¯tÃ©
- Structure: Organisation logique
- Contexte: Information pertinente
- SpÃ©cificitÃ©: DÃ©tails prÃ©cis
- Format: Description claire du format de sortie

## Format universel

```
# Contexte
[Contexte projet pertinent]

# Objectif
[Demande principale claire]

# Exigences
1. [Exigence 1]
2. [Exigence 2]
3. [Exigence 3]

# Contraintes
- [Contrainte 1]
- [Contrainte 2]

# Format de sortie
[Description prÃ©cise]
```

RÃ©ponds UNIQUEMENT avec le prompt reformatÃ©.
Utilise la mÃªme langue que le prompt original."""


# ============================================
# Modificateurs de style
# ============================================

STYLE_MODIFIERS = {
    PromptStyle.CONCIS: """
## Style: CONCIS
- Va droit au but, phrases courtes
- Maximum 150 mots
- Uniquement l'essentiel""",
    
    PromptStyle.DETAILLE: """
## Style: DÃ‰TAILLÃ‰
- Contexte complet avec exemples
- DÃ©taille chaque exigence
- Anticipe les edge cases""",
    
    PromptStyle.TECHNIQUE: """
## Style: TECHNIQUE
- Focus aspects techniques
- SpÃ©cifications prÃ©cises (versions, frameworks)
- Vocabulaire technique appropriÃ©""",
    
    PromptStyle.CREATIF: """
## Style: CRÃ‰ATIF
- Place Ã  l'interprÃ©tation
- Encourage l'originalitÃ©
- Focus sur l'intention"""
}


# ============================================
# Fonctions utilitaires
# ============================================

SYSTEM_PROMPTS = {
    TargetModel.CLAUDE_OPUS_4_5: SYSTEM_PROMPT_CLAUDE_OPUS,
    TargetModel.CLAUDE_SONNET_4_5: SYSTEM_PROMPT_CLAUDE_SONNET,
    TargetModel.CLAUDE_HAIKU_4_5: SYSTEM_PROMPT_CLAUDE_HAIKU,
    TargetModel.GPT_5_1: SYSTEM_PROMPT_GPT_5_1,
    TargetModel.GPT_5_1_MINI: SYSTEM_PROMPT_GPT_5_1_MINI,
    TargetModel.GPT_5_PRO: SYSTEM_PROMPT_GPT_5_PRO,
    TargetModel.GEMINI_3_PRO: SYSTEM_PROMPT_GEMINI_3_PRO,
    TargetModel.GEMINI_3_FLASH: SYSTEM_PROMPT_GEMINI_3_FLASH,
    TargetModel.UNIVERSAL: SYSTEM_PROMPT_UNIVERSAL,
}


def get_system_prompt(target: TargetModel) -> str:
    """Retourne le system prompt pour le modÃ¨le cible."""
    return SYSTEM_PROMPTS.get(target, SYSTEM_PROMPT_UNIVERSAL)


def get_style_modifier(style: PromptStyle) -> str:
    """Retourne le modificateur de style."""
    return STYLE_MODIFIERS.get(style, "")


def build_reformat_prompt(
    raw_prompt: str,
    project_context: str,
    profile: ReformatProfile
) -> tuple[str, str]:
    """Construit le prompt complet pour le reformatage."""
    system_prompt = get_system_prompt(profile.target_model)
    style_modifier = get_style_modifier(profile.style)
    if style_modifier:
        system_prompt += "\n" + style_modifier
    
    user_prompt = f"""## Contexte du projet
{project_context}

## Prompt original Ã  reformater
{raw_prompt}

## Instructions
Reformate ce prompt en suivant les guidelines pour {profile.target_model.value}.
"""
    
    if profile.include_examples:
        user_prompt += "Inclus un exemple si pertinent.\n"
    if not profile.include_constraints:
        user_prompt += "N'inclus pas de section contraintes.\n"
    if not profile.include_output_format:
        user_prompt += "N'inclus pas de section format de sortie.\n"
    
    return system_prompt, user_prompt


# ============================================
# Profils prÃ©dÃ©finis
# ============================================

PRESET_PROFILES = {
    # Claude (Anthropic)
    "claude_opus_4.5": ReformatProfile(
        target_model=TargetModel.CLAUDE_OPUS_4_5,
        style=PromptStyle.DETAILLE,
        include_examples=True,
        pricing=MODEL_PRICING[TargetModel.CLAUDE_OPUS_4_5]
    ),
    "claude_sonnet_4.5": ReformatProfile(
        target_model=TargetModel.CLAUDE_SONNET_4_5,
        style=PromptStyle.TECHNIQUE,
        include_examples=True,
        pricing=MODEL_PRICING[TargetModel.CLAUDE_SONNET_4_5]
    ),
    "claude_haiku_4.5": ReformatProfile(
        target_model=TargetModel.CLAUDE_HAIKU_4_5,
        style=PromptStyle.CONCIS,
        include_examples=False,
        pricing=MODEL_PRICING[TargetModel.CLAUDE_HAIKU_4_5]
    ),
    
    # GPT (OpenAI)
    "gpt_5.1": ReformatProfile(
        target_model=TargetModel.GPT_5_1,
        style=PromptStyle.DETAILLE,
        include_examples=True,
        pricing=MODEL_PRICING[TargetModel.GPT_5_1]
    ),
    "gpt_5.1_mini": ReformatProfile(
        target_model=TargetModel.GPT_5_1_MINI,
        style=PromptStyle.CONCIS,
        include_examples=False,
        pricing=MODEL_PRICING[TargetModel.GPT_5_1_MINI]
    ),
    "gpt_5_pro": ReformatProfile(
        target_model=TargetModel.GPT_5_PRO,
        style=PromptStyle.DETAILLE,
        include_examples=True,
        pricing=MODEL_PRICING[TargetModel.GPT_5_PRO]
    ),
    
    # Gemini (Google)
    "gemini_3_pro": ReformatProfile(
        target_model=TargetModel.GEMINI_3_PRO,
        style=PromptStyle.DETAILLE,
        include_examples=True,
        pricing=MODEL_PRICING[TargetModel.GEMINI_3_PRO]
    ),
    "gemini_3_flash": ReformatProfile(
        target_model=TargetModel.GEMINI_3_FLASH,
        style=PromptStyle.CONCIS,
        include_examples=False,
        pricing=MODEL_PRICING[TargetModel.GEMINI_3_FLASH]
    ),
    
    # Universel
    "universel": ReformatProfile(
        target_model=TargetModel.UNIVERSAL,
        style=PromptStyle.DETAILLE,
        include_examples=True,
        pricing=MODEL_PRICING[TargetModel.UNIVERSAL]
    ),
}


def get_profile(name: str) -> ReformatProfile:
    """RÃ©cupÃ¨re un profil prÃ©dÃ©fini."""
    return PRESET_PROFILES.get(name, PRESET_PROFILES["universel"])


def list_profiles() -> list[str]:
    """Liste les noms des profils disponibles."""
    return list(PRESET_PROFILES.keys())


def get_pricing(model: TargetModel) -> ModelPricing:
    """RÃ©cupÃ¨re le pricing d'un modÃ¨le."""
    return MODEL_PRICING.get(model, MODEL_PRICING[TargetModel.UNIVERSAL])


# ============================================
# Comparaison des modÃ¨les
# ============================================

def compare_models(input_tokens: int = 1000, output_tokens: int = 500) -> list[dict]:
    """
    Compare les modÃ¨les par prix et caractÃ©ristiques.
    
    Args:
        input_tokens: Nombre de tokens en entrÃ©e pour le calcul
        output_tokens: Nombre de tokens en sortie pour le calcul
    
    Returns:
        Liste triÃ©e par coÃ»t (moins cher en premier)
    """
    comparisons = []
    
    for model, pricing in MODEL_PRICING.items():
        cost = pricing.estimate_cost(input_tokens, output_tokens)
        comparisons.append({
            "model": model.value,
            "cost": cost,
            "cost_display": f"${cost:.4f}",
            "input_price": f"${pricing.input_price}/M",
            "output_price": f"${pricing.output_price}/M",
            "context": f"{pricing.context_window // 1000}K",
            "tier": _get_model_tier(model),
        })
    
    return sorted(comparisons, key=lambda x: x["cost"])


def _get_model_tier(model: TargetModel) -> str:
    """Retourne le tier de performance du modÃ¨le."""
    premium = [TargetModel.CLAUDE_OPUS_4_5, TargetModel.GPT_5_PRO, TargetModel.GPT_5_1]
    mid = [TargetModel.CLAUDE_SONNET_4_5, TargetModel.GEMINI_3_PRO]
    
    if model in premium:
        return "ğŸ”¥ Premium"
    elif model in mid:
        return "âš¡ Performant"
    else:
        return "ğŸ’° Ã‰conomique"


def get_recommendation(task_type: str) -> dict:
    """
    Recommande un modÃ¨le selon le type de tÃ¢che.
    
    Args:
        task_type: "code", "chat", "analysis", "creative", "budget"
    
    Returns:
        Dictionnaire avec recommandation et alternatives
    """
    recommendations = {
        "code": {
            "best": "claude_opus_4.5",
            "balanced": "claude_sonnet_4.5",
            "budget": "gpt_5.1_mini",
            "reason": "Opus 4.5 excelle en code complexe, Sonnet pour le quotidien"
        },
        "chat": {
            "best": "gpt_5.1",
            "balanced": "gemini_3_flash",
            "budget": "claude_haiku_4.5",
            "reason": "GPT-5.1 est plus naturel, Flash/Haiku pour le volume"
        },
        "analysis": {
            "best": "gemini_3_pro",
            "balanced": "claude_sonnet_4.5",
            "budget": "gpt_5.1_mini",
            "reason": "Gemini 3 Pro avec 1M tokens pour les gros documents"
        },
        "creative": {
            "best": "gpt_5.1",
            "balanced": "claude_sonnet_4.5",
            "budget": "gemini_3_flash",
            "reason": "GPT-5.1 moins sycophantique, plus crÃ©atif"
        },
        "budget": {
            "best": "gpt_5.1_mini",
            "balanced": "claude_haiku_4.5",
            "budget": "gemini_3_flash",
            "reason": "GPT-5.1 Mini offre le meilleur rapport qualitÃ©/prix"
        },
    }
    
    return recommendations.get(task_type, recommendations["budget"])


def format_comparison_table() -> str:
    """GÃ©nÃ¨re un tableau de comparaison formatÃ©."""
    comparisons = compare_models()
    
    lines = [
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
        "â”‚ ModÃ¨le               â”‚ Input/M       â”‚ Output/M      â”‚ Contexte â”‚ Tier        â”‚",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤",
    ]
    
    for c in comparisons:
        model = c["model"][:20].ljust(20)
        input_p = c["input_price"].ljust(13)
        output_p = c["output_price"].ljust(13)
        context = c["context"].ljust(8)
        tier = c["tier"].ljust(11)
        lines.append(f"â”‚ {model} â”‚ {input_p} â”‚ {output_p} â”‚ {context} â”‚ {tier} â”‚")
    
    lines.append("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    return "\n".join(lines)
